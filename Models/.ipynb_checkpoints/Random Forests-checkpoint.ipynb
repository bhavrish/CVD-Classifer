{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d7cb74",
   "metadata": {
    "id": "13d7cb74"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf0b048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baf0b048",
    "outputId": "d17bf3c1-7ea1-4827-c02d-85cf19284982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  \\\n",
      "0           1   1  20228       1     156    85.0    140     90            3   \n",
      "1           3   3  17623       2     169    82.0    150    100            1   \n",
      "2           6   9  22113       1     157    93.0    130     80            3   \n",
      "3          22  32  23046       1     158    90.0    145     85            2   \n",
      "4          24  35  16608       1     170    68.0    150     90            3   \n",
      "\n",
      "   gluc  smoke  alco  active  cardio  \n",
      "0     1      0     0       1       1  \n",
      "1     1      0     0       1       1  \n",
      "2     1      0     0       1       0  \n",
      "3     2      0     0       1       1  \n",
      "4     1      0     0       1       1  \n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('../Data/Cluster0', delimiter=',')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29330ff7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "29330ff7",
    "outputId": "b67f233d-2a38-4a1f-a6c0-6f59c65ba8a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22113</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>93.0</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23046</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>90.0</td>\n",
       "      <td>145</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16608</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>68.0</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0  20228       1     156    85.0    140     90            3     1      0   \n",
       "1  17623       2     169    82.0    150    100            1     1      0   \n",
       "2  22113       1     157    93.0    130     80            3     1      0   \n",
       "3  23046       1     158    90.0    145     85            2     2      0   \n",
       "4  16608       1     170    68.0    150     90            3     1      0   \n",
       "\n",
       "   alco  active  \n",
       "0     0       1  \n",
       "1     0       1  \n",
       "2     0       1  \n",
       "3     0       1  \n",
       "4     0       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = df.iloc[:,2:-1] # eventhing but last column\n",
    "y = df.iloc[:,-1] #last column\n",
    "\n",
    "df_feat.head() # age is in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47069da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f47069da",
    "outputId": "1f576bc4-c758-41f6-b087-2e9337928d32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.059287 -0.359705 -0.775476  0.387964 -0.380744 -0.113539  1.853778   \n",
      "1     -1.240112  2.780053  1.118664  0.190938  0.303304  1.161563 -0.729477   \n",
      "2      0.795168 -0.359705 -0.629773  0.913367 -1.064791 -1.388641  1.853778   \n",
      "3      1.218089 -0.359705 -0.484070  0.716341 -0.038720 -0.751090  0.562151   \n",
      "4     -1.700203 -0.359705  1.264367 -0.728518  0.303304 -0.113539  1.853778   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "13411 -0.710214 -0.359705 -0.046960 -1.516624  1.671399 -0.113539 -0.729477   \n",
      "13412 -0.299078 -0.359705  1.555773 -0.597168 -1.064791 -0.113539 -0.729477   \n",
      "13413  0.324198 -0.359705  0.535852  0.059587  0.303304 -1.388641 -0.729477   \n",
      "13414  1.016374 -0.359705 -0.484070  3.080656 -0.380744 -0.113539  0.562151   \n",
      "13415  0.939315 -0.359705  0.244446 -0.465817 -0.722767 -1.388641 -0.729477   \n",
      "\n",
      "             7         8         9         10  \n",
      "0     -0.312438 -0.034555 -0.008634  0.491343  \n",
      "1     -0.312438 -0.034555 -0.008634  0.491343  \n",
      "2     -0.312438 -0.034555 -0.008634  0.491343  \n",
      "3      3.160357 -0.034555 -0.008634  0.491343  \n",
      "4     -0.312438 -0.034555 -0.008634  0.491343  \n",
      "...         ...       ...       ...       ...  \n",
      "13411 -0.312438 -0.034555 -0.008634  0.491343  \n",
      "13412 -0.312438 -0.034555 -0.008634  0.491343  \n",
      "13413 -0.312438 -0.034555 -0.008634  0.491343  \n",
      "13414  3.160357 -0.034555 -0.008634  0.491343  \n",
      "13415  3.160357 -0.034555 -0.008634 -2.035240  \n",
      "\n",
      "[13416 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# standardize: mean = 0, std = 1\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_feat)\n",
    "X = pd.DataFrame(scaled_features)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3965f6",
   "metadata": {
    "id": "db3965f6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fc72c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "54fc72c2",
    "outputId": "9e1eac5c-e13d-4f26-b73c-73d550e14b17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  1 ;\tAccuracy: 0.7021466905187835\n",
      "0.37040349831047503\n",
      "---\n",
      "i:  2 ;\tAccuracy: 0.6186642814549791\n",
      "0.4814152256012721\n",
      "---\n",
      "i:  3 ;\tAccuracy: 0.7504472271914132\n",
      "0.556450009938382\n",
      "---\n",
      "i:  4 ;\tAccuracy: 0.7021466905187835\n",
      "0.6085271317829457\n",
      "---\n",
      "i:  5 ;\tAccuracy: 0.7713178294573644\n",
      "0.6294971178692109\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  6 ;\tAccuracy: 0.753726893261777\n",
      "0.6606042536275094\n",
      "---\n",
      "i:  7 ;\tAccuracy: 0.7784734645199761\n",
      "0.684257602862254\n",
      "---\n",
      "i:  8 ;\tAccuracy: 0.7635658914728682\n",
      "0.7006559332140727\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  9 ;\tAccuracy: 0.7844364937388193\n",
      "0.7028423772609819\n",
      "---\n",
      "i:  10 ;\tAccuracy: 0.7731067382230173\n",
      "0.716259193003379\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  11 ;\tAccuracy: 0.7898032200357782\n",
      "0.725402504472272\n",
      "---\n",
      "i:  12 ;\tAccuracy: 0.7769827072152653\n",
      "0.7356390379646194\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  13 ;\tAccuracy: 0.7942754919499105\n",
      "0.7441860465116279\n",
      "---\n",
      "i:  14 ;\tAccuracy: 0.7832438878950507\n",
      "0.7395150069568674\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  15 ;\tAccuracy: 0.7921884317233154\n",
      "0.7402106936990658\n",
      "---\n",
      "i:  16 ;\tAccuracy: 0.7832438878950507\n",
      "0.7513416815742398\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  17 ;\tAccuracy: 0.7921884317233154\n",
      "0.7495527728085868\n",
      "---\n",
      "i:  18 ;\tAccuracy: 0.7889087656529516\n",
      "0.7548201152852315\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  19 ;\tAccuracy: 0.7936791890280263\n",
      "0.7615782150665872\n",
      "---\n",
      "i:  20 ;\tAccuracy: 0.7903995229576625\n",
      "0.7588948519181077\n",
      "---\n",
      "i:  21 ;\tAccuracy: 0.7948717948717948\n",
      "0.7625720532697277\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  decision = (predictions[k] /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  22 ;\tAccuracy: 0.7862254025044723\n",
      "0.7637646591134963\n",
      "---\n",
      "i:  23 ;\tAccuracy: 0.7954680977936792\n",
      "0.7700258397932817\n",
      "---\n",
      "i:  24 ;\tAccuracy: 0.7948717948717948\n",
      "0.7728085867620751\n",
      "---\n",
      "i:  25 ;\tAccuracy: 0.7942754919499105\n",
      "0.7704233750745378\n",
      "---\n",
      "i:  26 ;\tAccuracy: 0.7903995229576625\n",
      "0.7776783939574637\n",
      "---\n",
      "i:  27 ;\tAccuracy: 0.7972570065593322\n",
      "0.7728085867620751\n",
      "---\n",
      "i:  28 ;\tAccuracy: 0.7954680977936792\n",
      "0.7722122838401908\n",
      "---\n",
      "i:  29 ;\tAccuracy: 0.7957662492546214\n",
      "0.7785728483402902\n",
      "---\n",
      "i:  30 ;\tAccuracy: 0.7957662492546214\n",
      "0.7734048896839595\n",
      "---\n",
      "i:  31 ;\tAccuracy: 0.7996422182468694\n",
      "0.7764857881136951\n",
      "---\n",
      "i:  32 ;\tAccuracy: 0.7868217054263565\n",
      "0.7788709998012323\n",
      "---\n",
      "i:  33 ;\tAccuracy: 0.7957662492546214\n",
      "0.7821506658715961\n",
      "---\n",
      "i:  34 ;\tAccuracy: 0.7966607036374478\n",
      "0.7808586762075134\n",
      "---\n",
      "i:  35 ;\tAccuracy: 0.8020274299344067\n",
      "0.7805605247465712\n",
      "---\n",
      "i:  36 ;\tAccuracy: 0.7889087656529516\n",
      "0.7805605247465712\n",
      "---\n",
      "i:  37 ;\tAccuracy: 0.7933810375670841\n",
      "0.7793679189028027\n",
      "---\n",
      "i:  38 ;\tAccuracy: 0.793082886106142\n",
      "0.7812562114887697\n",
      "---\n",
      "i:  39 ;\tAccuracy: 0.7963625521765056\n",
      "0.7852315643013318\n",
      "---\n",
      "i:  40 ;\tAccuracy: 0.8002385211687537\n",
      "0.7850327966607037\n",
      "---\n",
      "i:  41 ;\tAccuracy: 0.7966607036374478\n",
      "0.7847346451997614\n",
      "---\n",
      "i:  42 ;\tAccuracy: 0.7984496124031008\n",
      "0.783740806996621\n",
      "---\n",
      "i:  43 ;\tAccuracy: 0.802623732856291\n",
      "0.7874180083482409\n",
      "---\n",
      "i:  44 ;\tAccuracy: 0.7966607036374478\n",
      "0.7839395746372491\n",
      "---\n",
      "i:  45 ;\tAccuracy: 0.7963625521765056\n",
      "0.7867223216060425\n",
      "---\n",
      "i:  46 ;\tAccuracy: 0.7945736434108527\n",
      "0.7855297157622739\n",
      "---\n",
      "i:  47 ;\tAccuracy: 0.8002385211687537\n",
      "0.7872192407076128\n",
      "---\n",
      "i:  48 ;\tAccuracy: 0.7969588550983899\n",
      "0.7848340290200755\n",
      "---\n",
      "i:  49 ;\tAccuracy: 0.8029218843172331\n",
      "0.7903001391373484\n",
      "---\n",
      "i:  50 ;\tAccuracy: 0.7999403697078116\n",
      "0.7855297157622739\n",
      "---\n",
      "i:  51 ;\tAccuracy: 0.8029218843172331\n",
      "0.7828463526137944\n",
      "---\n",
      "i:  52 ;\tAccuracy: 0.7996422182468694\n",
      "0.7870204730669847\n",
      "---\n",
      "i:  53 ;\tAccuracy: 0.7969588550983899\n",
      "0.7879149274498112\n",
      "---\n",
      "i:  54 ;\tAccuracy: 0.7996422182468694\n",
      "0.7880143112701252\n",
      "---\n",
      "i:  55 ;\tAccuracy: 0.7996422182468694\n",
      "0.7882130789107533\n",
      "---\n",
      "i:  56 ;\tAccuracy: 0.7981514609421586\n",
      "0.7879149274498112\n",
      "---\n",
      "i:  57 ;\tAccuracy: 0.7996422182468694\n",
      "0.7883124627310674\n",
      "---\n",
      "i:  58 ;\tAccuracy: 0.7987477638640429\n",
      "0.7862254025044723\n",
      "---\n",
      "i:  59 ;\tAccuracy: 0.8017292784734645\n",
      "0.7896044523951501\n",
      "---\n",
      "i:  60 ;\tAccuracy: 0.7960644007155635\n",
      "0.7863247863247863\n",
      "---\n",
      "i:  61 ;\tAccuracy: 0.8005366726296959\n",
      "0.7883124627310674\n",
      "---\n",
      "i:  62 ;\tAccuracy: 0.8002385211687537\n",
      "0.7891075332935799\n",
      "---\n",
      "i:  63 ;\tAccuracy: 0.7978533094812165\n",
      "0.7903995229576625\n",
      "---\n",
      "i:  64 ;\tAccuracy: 0.7999403697078116\n",
      "0.7905982905982906\n",
      "---\n",
      "i:  65 ;\tAccuracy: 0.8011329755515803\n",
      "0.7907970582389187\n",
      "---\n",
      "i:  66 ;\tAccuracy: 0.7999403697078116\n",
      "0.7915921288014311\n",
      "---\n",
      "i:  67 ;\tAccuracy: 0.8035181872391175\n",
      "0.7879149274498112\n",
      "---\n",
      "i:  68 ;\tAccuracy: 0.7999403697078116\n",
      "0.7901013714967203\n",
      "---\n",
      "i:  69 ;\tAccuracy: 0.8002385211687537\n",
      "0.791293977340489\n",
      "---\n",
      "i:  70 ;\tAccuracy: 0.7996422182468694\n",
      "0.7892069171138939\n",
      "---\n",
      "i:  71 ;\tAccuracy: 0.7954680977936792\n",
      "0.7887099980123236\n",
      "---\n",
      "i:  72 ;\tAccuracy: 0.802623732856291\n",
      "0.7896044523951501\n",
      "---\n",
      "i:  73 ;\tAccuracy: 0.7996422182468694\n",
      "0.7902007553170344\n",
      "---\n",
      "i:  74 ;\tAccuracy: 0.8011329755515803\n",
      "0.7904989067779765\n",
      "---\n",
      "i:  75 ;\tAccuracy: 0.8002385211687537\n",
      "0.7906976744186046\n",
      "---\n",
      "i:  76 ;\tAccuracy: 0.7990459153249851\n",
      "0.7932816537467701\n",
      "---\n",
      "i:  77 ;\tAccuracy: 0.8002385211687537\n",
      "0.7896044523951501\n",
      "---\n",
      "i:  78 ;\tAccuracy: 0.8014311270125224\n",
      "0.7898032200357782\n",
      "---\n",
      "i:  79 ;\tAccuracy: 0.8014311270125224\n",
      "0.7898032200357782\n",
      "---\n",
      "i:  80 ;\tAccuracy: 0.7975551580202743\n",
      "0.7886106141920095\n",
      "---\n",
      "i:  81 ;\tAccuracy: 0.8020274299344067\n",
      "0.791393361160803\n",
      "---\n",
      "i:  82 ;\tAccuracy: 0.8020274299344067\n",
      "0.7936791890280263\n",
      "---\n",
      "i:  83 ;\tAccuracy: 0.7993440667859273\n",
      "0.7891075332935799\n",
      "---\n",
      "i:  84 ;\tAccuracy: 0.8032200357781754\n",
      "0.791293977340489\n",
      "---\n",
      "i:  85 ;\tAccuracy: 0.7966607036374478\n",
      "0.793082886106142\n",
      "---\n",
      "i:  86 ;\tAccuracy: 0.8038163387000596\n",
      "0.7891075332935799\n",
      "---\n",
      "i:  87 ;\tAccuracy: 0.8029218843172331\n",
      "0.791194593520175\n",
      "---\n",
      "i:  88 ;\tAccuracy: 0.7996422182468694\n",
      "0.7934804213873982\n",
      "---\n",
      "i:  89 ;\tAccuracy: 0.8032200357781754\n",
      "0.7920890479030014\n",
      "---\n",
      "i:  90 ;\tAccuracy: 0.8029218843172331\n",
      "0.791194593520175\n",
      "---\n",
      "i:  91 ;\tAccuracy: 0.7996422182468694\n",
      "0.789306300934208\n",
      "---\n",
      "i:  92 ;\tAccuracy: 0.8002385211687537\n",
      "0.7877161598091831\n",
      "---\n",
      "i:  93 ;\tAccuracy: 0.8023255813953488\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  94 ;\tAccuracy: 0.8029218843172331\n",
      "0.7887099980123236\n",
      "---\n",
      "i:  95 ;\tAccuracy: 0.7996422182468694\n",
      "0.7919896640826873\n",
      "---\n",
      "i:  96 ;\tAccuracy: 0.8020274299344067\n",
      "0.7917908964420592\n",
      "---\n",
      "i:  97 ;\tAccuracy: 0.8014311270125224\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  98 ;\tAccuracy: 0.7993440667859273\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  99 ;\tAccuracy: 0.8005366726296959\n",
      "0.7909958258795468\n",
      "---\n",
      "i:  100 ;\tAccuracy: 0.7954680977936792\n",
      "0.7906976744186046\n",
      "---\n",
      "i:  101 ;\tAccuracy: 0.7975551580202743\n",
      "0.793182269926456\n",
      "---\n",
      "i:  102 ;\tAccuracy: 0.8005366726296959\n",
      "0.7906976744186046\n",
      "---\n",
      "i:  103 ;\tAccuracy: 0.8029218843172331\n",
      "0.7964619359968197\n",
      "---\n",
      "i:  104 ;\tAccuracy: 0.7981514609421586\n",
      "0.7923871993639435\n",
      "---\n",
      "i:  105 ;\tAccuracy: 0.7996422182468694\n",
      "0.7933810375670841\n",
      "---\n",
      "i:  106 ;\tAccuracy: 0.8020274299344067\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  107 ;\tAccuracy: 0.8029218843172331\n",
      "0.7928841184655138\n",
      "---\n",
      "i:  108 ;\tAccuracy: 0.8014311270125224\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  109 ;\tAccuracy: 0.800834824090638\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  110 ;\tAccuracy: 0.7993440667859273\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  111 ;\tAccuracy: 0.8017292784734645\n",
      "0.7926853508248857\n",
      "---\n",
      "i:  112 ;\tAccuracy: 0.7987477638640429\n",
      "0.7924865831842576\n",
      "---\n",
      "i:  113 ;\tAccuracy: 0.7999403697078116\n",
      "0.7909958258795468\n",
      "---\n",
      "i:  114 ;\tAccuracy: 0.8023255813953488\n",
      "0.7935798052077122\n",
      "---\n",
      "i:  115 ;\tAccuracy: 0.7987477638640429\n",
      "0.791293977340489\n",
      "---\n",
      "i:  116 ;\tAccuracy: 0.8014311270125224\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  117 ;\tAccuracy: 0.800834824090638\n",
      "0.7943748757702246\n",
      "---\n",
      "i:  118 ;\tAccuracy: 0.7996422182468694\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  119 ;\tAccuracy: 0.7999403697078116\n",
      "0.7932816537467701\n",
      "---\n",
      "i:  120 ;\tAccuracy: 0.7984496124031008\n",
      "0.7927847346451997\n",
      "---\n",
      "i:  121 ;\tAccuracy: 0.7999403697078116\n",
      "0.7929835022858278\n",
      "---\n",
      "i:  122 ;\tAccuracy: 0.7978533094812165\n",
      "0.7907970582389187\n",
      "---\n",
      "i:  123 ;\tAccuracy: 0.8005366726296959\n",
      "0.7915921288014311\n",
      "---\n",
      "i:  124 ;\tAccuracy: 0.8002385211687537\n",
      "0.7919896640826873\n",
      "---\n",
      "i:  125 ;\tAccuracy: 0.802623732856291\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  126 ;\tAccuracy: 0.7975551580202743\n",
      "0.7919896640826873\n",
      "---\n",
      "i:  127 ;\tAccuracy: 0.8005366726296959\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  128 ;\tAccuracy: 0.7981514609421586\n",
      "0.7943748757702246\n",
      "---\n",
      "i:  129 ;\tAccuracy: 0.7996422182468694\n",
      "0.7933810375670841\n",
      "---\n",
      "i:  130 ;\tAccuracy: 0.8017292784734645\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  131 ;\tAccuracy: 0.8014311270125224\n",
      "0.7932816537467701\n",
      "---\n",
      "i:  132 ;\tAccuracy: 0.8020274299344067\n",
      "0.7942754919499105\n",
      "---\n",
      "i:  133 ;\tAccuracy: 0.8011329755515803\n",
      "0.7909958258795468\n",
      "---\n",
      "i:  134 ;\tAccuracy: 0.8029218843172331\n",
      "0.7926853508248857\n",
      "---\n",
      "i:  135 ;\tAccuracy: 0.8005366726296959\n",
      "0.7923871993639435\n",
      "---\n",
      "i:  136 ;\tAccuracy: 0.800834824090638\n",
      "0.7946730272311667\n",
      "---\n",
      "i:  137 ;\tAccuracy: 0.7984496124031008\n",
      "0.7922878155436295\n",
      "---\n",
      "i:  138 ;\tAccuracy: 0.8023255813953488\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  139 ;\tAccuracy: 0.7990459153249851\n",
      "0.791293977340489\n",
      "---\n",
      "i:  140 ;\tAccuracy: 0.802623732856291\n",
      "0.7917908964420592\n",
      "---\n",
      "i:  141 ;\tAccuracy: 0.8014311270125224\n",
      "0.7926853508248857\n",
      "---\n",
      "i:  142 ;\tAccuracy: 0.7966607036374478\n",
      "0.7920890479030014\n",
      "---\n",
      "i:  143 ;\tAccuracy: 0.8032200357781754\n",
      "0.7933810375670841\n",
      "---\n",
      "i:  144 ;\tAccuracy: 0.8032200357781754\n",
      "0.7922878155436295\n",
      "---\n",
      "i:  145 ;\tAccuracy: 0.8023255813953488\n",
      "0.7924865831842576\n",
      "---\n",
      "i:  146 ;\tAccuracy: 0.7999403697078116\n",
      "0.7946730272311667\n",
      "---\n",
      "i:  147 ;\tAccuracy: 0.7993440667859273\n",
      "0.7929835022858278\n",
      "---\n",
      "i:  148 ;\tAccuracy: 0.8011329755515803\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  149 ;\tAccuracy: 0.800834824090638\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  150 ;\tAccuracy: 0.8023255813953488\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  151 ;\tAccuracy: 0.8038163387000596\n",
      "0.793182269926456\n",
      "---\n",
      "i:  152 ;\tAccuracy: 0.8035181872391175\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  153 ;\tAccuracy: 0.800834824090638\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  154 ;\tAccuracy: 0.800834824090638\n",
      "0.795070562512423\n",
      "---\n",
      "i:  155 ;\tAccuracy: 0.8020274299344067\n",
      "0.7935798052077122\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  156 ;\tAccuracy: 0.8002385211687537\n",
      "0.7935798052077122\n",
      "---\n",
      "i:  157 ;\tAccuracy: 0.802623732856291\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  158 ;\tAccuracy: 0.8029218843172331\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  159 ;\tAccuracy: 0.8023255813953488\n",
      "0.7973563903796462\n",
      "---\n",
      "i:  160 ;\tAccuracy: 0.7981514609421586\n",
      "0.7963625521765056\n",
      "---\n",
      "i:  161 ;\tAccuracy: 0.8023255813953488\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  162 ;\tAccuracy: 0.8029218843172331\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  163 ;\tAccuracy: 0.8029218843172331\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  164 ;\tAccuracy: 0.800834824090638\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  165 ;\tAccuracy: 0.7996422182468694\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  166 ;\tAccuracy: 0.8032200357781754\n",
      "0.795070562512423\n",
      "---\n",
      "i:  167 ;\tAccuracy: 0.802623732856291\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  168 ;\tAccuracy: 0.8032200357781754\n",
      "0.7938779566686543\n",
      "---\n",
      "i:  169 ;\tAccuracy: 0.8017292784734645\n",
      "0.7965613198171337\n",
      "---\n",
      "i:  170 ;\tAccuracy: 0.8017292784734645\n",
      "0.7936791890280263\n",
      "---\n",
      "i:  171 ;\tAccuracy: 0.8017292784734645\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  172 ;\tAccuracy: 0.8023255813953488\n",
      "0.7920890479030014\n",
      "---\n",
      "i:  173 ;\tAccuracy: 0.8014311270125224\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  174 ;\tAccuracy: 0.8020274299344067\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  175 ;\tAccuracy: 0.800834824090638\n",
      "0.7922878155436295\n",
      "---\n",
      "i:  176 ;\tAccuracy: 0.8035181872391175\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  177 ;\tAccuracy: 0.8002385211687537\n",
      "0.795070562512423\n",
      "---\n",
      "i:  178 ;\tAccuracy: 0.8020274299344067\n",
      "0.7927847346451997\n",
      "---\n",
      "i:  179 ;\tAccuracy: 0.8017292784734645\n",
      "0.7943748757702246\n",
      "---\n",
      "i:  180 ;\tAccuracy: 0.8014311270125224\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  181 ;\tAccuracy: 0.7999403697078116\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  182 ;\tAccuracy: 0.800834824090638\n",
      "0.7926853508248857\n",
      "---\n",
      "i:  183 ;\tAccuracy: 0.8035181872391175\n",
      "0.7933810375670841\n",
      "---\n",
      "i:  184 ;\tAccuracy: 0.8005366726296959\n",
      "0.793182269926456\n",
      "---\n",
      "i:  185 ;\tAccuracy: 0.7990459153249851\n",
      "0.7929835022858278\n",
      "---\n",
      "i:  186 ;\tAccuracy: 0.8032200357781754\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  187 ;\tAccuracy: 0.8002385211687537\n",
      "0.7946730272311667\n",
      "---\n",
      "i:  188 ;\tAccuracy: 0.8017292784734645\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  189 ;\tAccuracy: 0.8044126416219439\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  190 ;\tAccuracy: 0.7990459153249851\n",
      "0.793182269926456\n",
      "---\n",
      "i:  191 ;\tAccuracy: 0.7996422182468694\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  192 ;\tAccuracy: 0.8020274299344067\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  193 ;\tAccuracy: 0.8011329755515803\n",
      "0.7935798052077122\n",
      "---\n",
      "i:  194 ;\tAccuracy: 0.7996422182468694\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  195 ;\tAccuracy: 0.8023255813953488\n",
      "0.7948717948717948\n",
      "---\n",
      "i:  196 ;\tAccuracy: 0.8023255813953488\n",
      "0.7927847346451997\n",
      "---\n",
      "i:  197 ;\tAccuracy: 0.8029218843172331\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  198 ;\tAccuracy: 0.8044126416219439\n",
      "0.7927847346451997\n",
      "---\n",
      "i:  199 ;\tAccuracy: 0.8005366726296959\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  200 ;\tAccuracy: 0.8002385211687537\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  201 ;\tAccuracy: 0.8002385211687537\n",
      "0.7934804213873982\n",
      "---\n",
      "i:  202 ;\tAccuracy: 0.8017292784734645\n",
      "0.7935798052077122\n",
      "---\n",
      "i:  203 ;\tAccuracy: 0.8041144901610018\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  204 ;\tAccuracy: 0.7990459153249851\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  205 ;\tAccuracy: 0.8032200357781754\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  206 ;\tAccuracy: 0.8050089445438283\n",
      "0.7970582389187041\n",
      "---\n",
      "i:  207 ;\tAccuracy: 0.8044126416219439\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  208 ;\tAccuracy: 0.7996422182468694\n",
      "0.7959650168952495\n",
      "---\n",
      "i:  209 ;\tAccuracy: 0.8005366726296959\n",
      "0.7959650168952495\n",
      "---\n",
      "i:  210 ;\tAccuracy: 0.7990459153249851\n",
      "0.7967600874577618\n",
      "---\n",
      "i:  211 ;\tAccuracy: 0.800834824090638\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  212 ;\tAccuracy: 0.8011329755515803\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  213 ;\tAccuracy: 0.7999403697078116\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  214 ;\tAccuracy: 0.8044126416219439\n",
      "0.7946730272311667\n",
      "---\n",
      "i:  215 ;\tAccuracy: 0.8002385211687537\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  216 ;\tAccuracy: 0.8032200357781754\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  217 ;\tAccuracy: 0.8011329755515803\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  218 ;\tAccuracy: 0.8011329755515803\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  219 ;\tAccuracy: 0.7999403697078116\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  220 ;\tAccuracy: 0.8011329755515803\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  221 ;\tAccuracy: 0.7990459153249851\n",
      "0.7957662492546214\n",
      "---\n",
      "i:  222 ;\tAccuracy: 0.8038163387000596\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  223 ;\tAccuracy: 0.8020274299344067\n",
      "0.793182269926456\n",
      "---\n",
      "i:  224 ;\tAccuracy: 0.800834824090638\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  225 ;\tAccuracy: 0.802623732856291\n",
      "0.7958656330749354\n",
      "---\n",
      "i:  226 ;\tAccuracy: 0.8047107930828861\n",
      "0.7964619359968197\n",
      "---\n",
      "i:  227 ;\tAccuracy: 0.8032200357781754\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  228 ;\tAccuracy: 0.8032200357781754\n",
      "0.7936791890280263\n",
      "---\n",
      "i:  229 ;\tAccuracy: 0.8011329755515803\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  230 ;\tAccuracy: 0.8035181872391175\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  231 ;\tAccuracy: 0.7993440667859273\n",
      "0.7948717948717948\n",
      "---\n",
      "i:  232 ;\tAccuracy: 0.8002385211687537\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  233 ;\tAccuracy: 0.8023255813953488\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  234 ;\tAccuracy: 0.8014311270125224\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  235 ;\tAccuracy: 0.8050089445438283\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  236 ;\tAccuracy: 0.8023255813953488\n",
      "0.7942754919499105\n",
      "---\n",
      "i:  237 ;\tAccuracy: 0.8017292784734645\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  238 ;\tAccuracy: 0.8020274299344067\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  239 ;\tAccuracy: 0.800834824090638\n",
      "0.795070562512423\n",
      "---\n",
      "i:  240 ;\tAccuracy: 0.8014311270125224\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  241 ;\tAccuracy: 0.7993440667859273\n",
      "0.7963625521765056\n",
      "---\n",
      "i:  242 ;\tAccuracy: 0.802623732856291\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  243 ;\tAccuracy: 0.8029218843172331\n",
      "0.7938779566686543\n",
      "---\n",
      "i:  244 ;\tAccuracy: 0.8017292784734645\n",
      "0.7942754919499105\n",
      "---\n",
      "i:  245 ;\tAccuracy: 0.800834824090638\n",
      "0.7959650168952495\n",
      "---\n",
      "i:  246 ;\tAccuracy: 0.8032200357781754\n",
      "0.7965613198171337\n",
      "---\n",
      "i:  247 ;\tAccuracy: 0.8020274299344067\n",
      "0.793182269926456\n",
      "---\n",
      "i:  248 ;\tAccuracy: 0.8017292784734645\n",
      "0.795070562512423\n",
      "---\n",
      "i:  249 ;\tAccuracy: 0.8020274299344067\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  250 ;\tAccuracy: 0.8014311270125224\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  251 ;\tAccuracy: 0.8020274299344067\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  252 ;\tAccuracy: 0.8038163387000596\n",
      "0.7983502285827867\n",
      "---\n",
      "i:  253 ;\tAccuracy: 0.8038163387000596\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  254 ;\tAccuracy: 0.8020274299344067\n",
      "0.7959650168952495\n",
      "---\n",
      "i:  255 ;\tAccuracy: 0.8023255813953488\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  256 ;\tAccuracy: 0.7993440667859273\n",
      "0.7936791890280263\n",
      "---\n",
      "i:  257 ;\tAccuracy: 0.8017292784734645\n",
      "0.7948717948717948\n",
      "---\n",
      "i:  258 ;\tAccuracy: 0.8038163387000596\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  259 ;\tAccuracy: 0.8020274299344067\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  260 ;\tAccuracy: 0.7990459153249851\n",
      "0.7974557741999603\n",
      "---\n",
      "i:  261 ;\tAccuracy: 0.8017292784734645\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  262 ;\tAccuracy: 0.8032200357781754\n",
      "0.7942754919499105\n",
      "---\n",
      "i:  263 ;\tAccuracy: 0.800834824090638\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  264 ;\tAccuracy: 0.8014311270125224\n",
      "0.7958656330749354\n",
      "---\n",
      "i:  265 ;\tAccuracy: 0.800834824090638\n",
      "0.7964619359968197\n",
      "---\n",
      "i:  266 ;\tAccuracy: 0.8020274299344067\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  267 ;\tAccuracy: 0.8002385211687537\n",
      "0.795070562512423\n",
      "---\n",
      "i:  268 ;\tAccuracy: 0.7993440667859273\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  269 ;\tAccuracy: 0.8005366726296959\n",
      "0.7943748757702246\n",
      "---\n",
      "i:  270 ;\tAccuracy: 0.8020274299344067\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  271 ;\tAccuracy: 0.802623732856291\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  272 ;\tAccuracy: 0.8002385211687537\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  273 ;\tAccuracy: 0.8020274299344067\n",
      "0.7957662492546214\n",
      "---\n",
      "i:  274 ;\tAccuracy: 0.8023255813953488\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  275 ;\tAccuracy: 0.8014311270125224\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  276 ;\tAccuracy: 0.8002385211687537\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  277 ;\tAccuracy: 0.8044126416219439\n",
      "0.7957662492546214\n",
      "---\n",
      "i:  278 ;\tAccuracy: 0.8032200357781754\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  279 ;\tAccuracy: 0.8005366726296959\n",
      "0.7937785728483403\n",
      "---\n",
      "i:  280 ;\tAccuracy: 0.8017292784734645\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  281 ;\tAccuracy: 0.8044126416219439\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  282 ;\tAccuracy: 0.8038163387000596\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  283 ;\tAccuracy: 0.8023255813953488\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  284 ;\tAccuracy: 0.8017292784734645\n",
      "0.7967600874577618\n",
      "---\n",
      "i:  285 ;\tAccuracy: 0.8038163387000596\n",
      "0.7971576227390181\n",
      "---\n",
      "i:  286 ;\tAccuracy: 0.8038163387000596\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  287 ;\tAccuracy: 0.8038163387000596\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  288 ;\tAccuracy: 0.800834824090638\n",
      "0.7958656330749354\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  289 ;\tAccuracy: 0.7993440667859273\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  290 ;\tAccuracy: 0.8038163387000596\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  291 ;\tAccuracy: 0.8017292784734645\n",
      "0.7968594712780759\n",
      "---\n",
      "i:  292 ;\tAccuracy: 0.8023255813953488\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  293 ;\tAccuracy: 0.8020274299344067\n",
      "0.7934804213873982\n",
      "---\n",
      "i:  294 ;\tAccuracy: 0.8011329755515803\n",
      "0.7945736434108527\n",
      "---\n",
      "i:  295 ;\tAccuracy: 0.802623732856291\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  296 ;\tAccuracy: 0.8005366726296959\n",
      "0.7938779566686543\n",
      "---\n",
      "i:  297 ;\tAccuracy: 0.8029218843172331\n",
      "0.7972570065593322\n",
      "---\n",
      "i:  298 ;\tAccuracy: 0.7996422182468694\n",
      "0.7968594712780759\n",
      "---\n",
      "i:  299 ;\tAccuracy: 0.8014311270125224\n",
      "0.7964619359968197\n",
      "---\n",
      "i:  300 ;\tAccuracy: 0.8020274299344067\n",
      "0.7963625521765056\n",
      "---\n",
      "i:  301 ;\tAccuracy: 0.8032200357781754\n",
      "0.7941761081295965\n",
      "---\n",
      "i:  302 ;\tAccuracy: 0.8029218843172331\n",
      "0.7967600874577618\n",
      "---\n",
      "i:  303 ;\tAccuracy: 0.8029218843172331\n",
      "0.7948717948717948\n",
      "---\n",
      "i:  304 ;\tAccuracy: 0.8038163387000596\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  305 ;\tAccuracy: 0.8014311270125224\n",
      "0.7934804213873982\n",
      "---\n",
      "i:  306 ;\tAccuracy: 0.8020274299344067\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  307 ;\tAccuracy: 0.8014311270125224\n",
      "0.7943748757702246\n",
      "---\n",
      "i:  308 ;\tAccuracy: 0.8005366726296959\n",
      "0.7974557741999603\n",
      "---\n",
      "i:  309 ;\tAccuracy: 0.802623732856291\n",
      "0.7966607036374478\n",
      "---\n",
      "i:  310 ;\tAccuracy: 0.8035181872391175\n",
      "0.7976545418405884\n",
      "---\n",
      "i:  311 ;\tAccuracy: 0.8017292784734645\n",
      "0.7958656330749354\n",
      "---\n",
      "i:  312 ;\tAccuracy: 0.7999403697078116\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  313 ;\tAccuracy: 0.802623732856291\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  314 ;\tAccuracy: 0.7993440667859273\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  315 ;\tAccuracy: 0.802623732856291\n",
      "0.7958656330749354\n",
      "---\n",
      "i:  316 ;\tAccuracy: 0.7999403697078116\n",
      "0.7959650168952495\n",
      "---\n",
      "i:  317 ;\tAccuracy: 0.8017292784734645\n",
      "0.7965613198171337\n",
      "---\n",
      "i:  318 ;\tAccuracy: 0.8011329755515803\n",
      "0.795070562512423\n",
      "---\n",
      "i:  319 ;\tAccuracy: 0.8029218843172331\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  320 ;\tAccuracy: 0.8020274299344067\n",
      "0.7963625521765056\n",
      "---\n",
      "i:  321 ;\tAccuracy: 0.8041144901610018\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  322 ;\tAccuracy: 0.8017292784734645\n",
      "0.7965613198171337\n",
      "---\n",
      "i:  323 ;\tAccuracy: 0.8023255813953488\n",
      "0.7966607036374478\n",
      "---\n",
      "i:  324 ;\tAccuracy: 0.8029218843172331\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  325 ;\tAccuracy: 0.7999403697078116\n",
      "0.7976545418405884\n",
      "---\n",
      "i:  326 ;\tAccuracy: 0.7987477638640429\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  327 ;\tAccuracy: 0.8032200357781754\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  328 ;\tAccuracy: 0.800834824090638\n",
      "0.7947724110514808\n",
      "---\n",
      "i:  329 ;\tAccuracy: 0.8014311270125224\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  330 ;\tAccuracy: 0.8002385211687537\n",
      "0.7936791890280263\n",
      "---\n",
      "i:  331 ;\tAccuracy: 0.8002385211687537\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  332 ;\tAccuracy: 0.800834824090638\n",
      "0.7981514609421586\n",
      "---\n",
      "i:  333 ;\tAccuracy: 0.8044126416219439\n",
      "0.7943748757702246\n",
      "---\n",
      "i:  334 ;\tAccuracy: 0.8044126416219439\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  335 ;\tAccuracy: 0.8032200357781754\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  336 ;\tAccuracy: 0.8047107930828861\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  337 ;\tAccuracy: 0.802623732856291\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  338 ;\tAccuracy: 0.8005366726296959\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  339 ;\tAccuracy: 0.8029218843172331\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  340 ;\tAccuracy: 0.8014311270125224\n",
      "0.7963625521765056\n",
      "---\n",
      "i:  341 ;\tAccuracy: 0.8035181872391175\n",
      "0.7935798052077122\n",
      "---\n",
      "i:  342 ;\tAccuracy: 0.8029218843172331\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  343 ;\tAccuracy: 0.8032200357781754\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  344 ;\tAccuracy: 0.802623732856291\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  345 ;\tAccuracy: 0.8032200357781754\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  346 ;\tAccuracy: 0.8002385211687537\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  347 ;\tAccuracy: 0.8011329755515803\n",
      "0.7953687139733652\n",
      "---\n",
      "i:  348 ;\tAccuracy: 0.8038163387000596\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  349 ;\tAccuracy: 0.8002385211687537\n",
      "0.7952693301530511\n",
      "---\n",
      "i:  350 ;\tAccuracy: 0.8038163387000596\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  351 ;\tAccuracy: 0.800834824090638\n",
      "0.7968594712780759\n",
      "---\n",
      "i:  352 ;\tAccuracy: 0.8014311270125224\n",
      "0.793082886106142\n",
      "---\n",
      "i:  353 ;\tAccuracy: 0.8014311270125224\n",
      "0.7970582389187041\n",
      "---\n",
      "i:  354 ;\tAccuracy: 0.8032200357781754\n",
      "0.7972570065593322\n",
      "---\n",
      "i:  355 ;\tAccuracy: 0.7996422182468694\n",
      "0.7959650168952495\n",
      "---\n",
      "i:  356 ;\tAccuracy: 0.8023255813953488\n",
      "0.7963625521765056\n",
      "---\n",
      "i:  357 ;\tAccuracy: 0.8014311270125224\n",
      "0.793082886106142\n",
      "---\n",
      "i:  358 ;\tAccuracy: 0.8041144901610018\n",
      "0.7946730272311667\n",
      "---\n",
      "i:  359 ;\tAccuracy: 0.8029218843172331\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  360 ;\tAccuracy: 0.8047107930828861\n",
      "0.7957662492546214\n",
      "---\n",
      "i:  361 ;\tAccuracy: 0.802623732856291\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  362 ;\tAccuracy: 0.802623732856291\n",
      "0.7958656330749354\n",
      "---\n",
      "i:  363 ;\tAccuracy: 0.8017292784734645\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  364 ;\tAccuracy: 0.8005366726296959\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  365 ;\tAccuracy: 0.8011329755515803\n",
      "0.795070562512423\n",
      "---\n",
      "i:  366 ;\tAccuracy: 0.8047107930828861\n",
      "0.7966607036374478\n",
      "---\n",
      "i:  367 ;\tAccuracy: 0.802623732856291\n",
      "0.7965613198171337\n",
      "---\n",
      "i:  368 ;\tAccuracy: 0.8035181872391175\n",
      "0.7944742595905386\n",
      "---\n",
      "i:  369 ;\tAccuracy: 0.8023255813953488\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  370 ;\tAccuracy: 0.8029218843172331\n",
      "0.7975551580202743\n",
      "---\n",
      "i:  371 ;\tAccuracy: 0.8029218843172331\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  372 ;\tAccuracy: 0.802623732856291\n",
      "0.7960644007155635\n",
      "---\n",
      "i:  373 ;\tAccuracy: 0.7993440667859273\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  374 ;\tAccuracy: 0.8017292784734645\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  375 ;\tAccuracy: 0.8050089445438283\n",
      "0.7949711786921089\n",
      "---\n",
      "i:  376 ;\tAccuracy: 0.8011329755515803\n",
      "0.7975551580202743\n",
      "---\n",
      "i:  377 ;\tAccuracy: 0.8002385211687537\n",
      "0.7981514609421586\n",
      "---\n",
      "i:  378 ;\tAccuracy: 0.8014311270125224\n",
      "0.7957662492546214\n",
      "---\n",
      "i:  379 ;\tAccuracy: 0.7999403697078116\n",
      "0.7967600874577618\n",
      "---\n",
      "i:  380 ;\tAccuracy: 0.8023255813953488\n",
      "0.7951699463327371\n",
      "---\n",
      "i:  381 ;\tAccuracy: 0.802623732856291\n",
      "0.795070562512423\n",
      "---\n",
      "i:  382 ;\tAccuracy: 0.8005366726296959\n",
      "0.7956668654343073\n",
      "---\n",
      "i:  383 ;\tAccuracy: 0.8014311270125224\n",
      "0.7969588550983899\n",
      "---\n",
      "i:  384 ;\tAccuracy: 0.7999403697078116\n",
      "0.7962631683561916\n",
      "---\n",
      "i:  385 ;\tAccuracy: 0.7999403697078116\n",
      "0.7971576227390181\n",
      "---\n",
      "i:  386 ;\tAccuracy: 0.8029218843172331\n",
      "0.7983502285827867\n",
      "---\n",
      "i:  387 ;\tAccuracy: 0.8011329755515803\n",
      "0.7954680977936792\n",
      "---\n",
      "i:  388 ;\tAccuracy: 0.8020274299344067\n",
      "0.7957662492546214\n",
      "---\n",
      "i:  389 ;\tAccuracy: 0.802623732856291\n",
      "0.7985489962234148\n",
      "---\n",
      "i:  390 ;\tAccuracy: 0.8020274299344067\n",
      "0.7969588550983899\n",
      "---\n",
      "i:  391 ;\tAccuracy: 0.8020274299344067\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  392 ;\tAccuracy: 0.8029218843172331\n",
      "0.7964619359968197\n",
      "---\n",
      "i:  393 ;\tAccuracy: 0.8023255813953488\n",
      "0.7940767243092824\n",
      "---\n",
      "i:  394 ;\tAccuracy: 0.8005366726296959\n",
      "0.795070562512423\n",
      "---\n",
      "i:  395 ;\tAccuracy: 0.8002385211687537\n",
      "0.7976545418405884\n",
      "---\n",
      "i:  396 ;\tAccuracy: 0.8047107930828861\n",
      "0.7961637845358776\n",
      "---\n",
      "i:  397 ;\tAccuracy: 0.7996422182468694\n",
      "0.7955674816139933\n",
      "---\n",
      "i:  398 ;\tAccuracy: 0.8014311270125224\n",
      "0.7939773404889684\n",
      "---\n",
      "i:  399 ;\tAccuracy: 0.8017292784734645\n",
      "0.7976545418405884\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "for i in range(1,400):\n",
    "    classifier = RandomForestClassifier(n_estimators=i, oob_score=True)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"i: \", i, \";\\tAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "#     print('Score: ', classifier.score(X_train, y_train))\n",
    "    print(classifier.oob_score_)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5scjFCSajcXN",
   "metadata": {
    "id": "5scjFCSajcXN"
   },
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    \"n_estimators\" : range(100, 300, 10),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_split': [1, 2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "P4ayGB56i6vY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4ayGB56i6vY",
    "outputId": "2e016a47-58b0-43fa-fe14-faaba2cb81a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.79725699 0.79815133\n",
      " 0.79745526 0.79785282 0.79745516 0.79745541 0.79616351 0.79884686\n",
      " 0.79695829 0.79695839 0.80063593 0.79974154 0.80013886 0.79864795\n",
      " 0.79974144 0.80162947 0.79974164 0.8012319  0.79884637 0.79904586\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80481014 0.8040152\n",
      " 0.80441267 0.80461148 0.80560502 0.80451217 0.80550581 0.80471073\n",
      " 0.80560517 0.8051081  0.8050086  0.80292157 0.8053072  0.80530681\n",
      " 0.80520745 0.80540631 0.80570452 0.80371684 0.80550571 0.80471068\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80540651 0.80679786\n",
      " 0.80620184 0.80640044 0.80659896 0.80630055 0.80659901 0.80630104\n",
      " 0.80649955 0.80689727 0.80610223 0.80580412 0.80550571 0.80689736\n",
      " 0.80461148 0.80689717 0.80749364 0.80630079 0.80610209 0.80590348\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.79785297 0.79715749\n",
      " 0.79805133 0.79864825 0.7971572  0.79745511 0.79735556 0.79765402\n",
      " 0.79725675 0.79775332 0.80133136 0.79834999 0.79904552 0.79894626\n",
      " 0.7986481  0.7973559  0.80013871 0.80013866 0.80033762 0.79805178\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80461148 0.80461143\n",
      " 0.80351819 0.80421396 0.80411426 0.80649955 0.80381615 0.80391545\n",
      " 0.80480974 0.80500914 0.80540651 0.80431322 0.80490944 0.80490959\n",
      " 0.80431322 0.8052076  0.80431322 0.80471068 0.80341878 0.80431327\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80649975 0.80620139\n",
      " 0.80580383 0.80540636 0.80659915 0.80709627 0.80630089 0.8064\n",
      " 0.80640035 0.80640015 0.80620154 0.80560507 0.80689717 0.80560551\n",
      " 0.80689722 0.80590348 0.80610233 0.80620169 0.80679796 0.80610228\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.79636251 0.79934343\n",
      " 0.7958649  0.79795223 0.7973559  0.79755476 0.79844924 0.79834989\n",
      " 0.79656092 0.79815128 0.80013832 0.80023816 0.8000395  0.7998406\n",
      " 0.80043697 0.80033737 0.80013871 0.80063558 0.80013866 0.79914512\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80540631 0.80471063\n",
      " 0.80550581 0.80570447 0.80490964 0.80560527 0.80490944 0.80480999\n",
      " 0.80431322 0.80510849 0.80431312 0.80500899 0.80490949 0.80451202\n",
      " 0.80411426 0.80490929 0.80560487 0.80421396 0.80550596 0.80580388\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80759255 0.80640015\n",
      " 0.80679786 0.80590333 0.80640044 0.80510854 0.80550601 0.80560522\n",
      " 0.80669841 0.8064003  0.80640054 0.80649985 0.80739433 0.80679821\n",
      " 0.80570457 0.80620144 0.80689731 0.80590338 0.8064997  0.80600263]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_features': 'log2',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RF = RandomForestClassifier(oob_score=True)\n",
    "gs = GridSearchCV(RF, grid_params, verbose= 1, n_jobs= -1)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0d5186",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b0d5186",
    "outputId": "ad0af45d-4ea9-4c1b-a586-fd2a5f81ca0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79693371 0.79703998 0.79469786 0.79672044\n",
      " 0.79618852 0.79863748 0.79640129 0.79714699 0.79650824 0.79757292\n",
      " 0.79778568 0.79853127 0.79725331 0.79757258 0.79661428 0.79789212\n",
      " 0.79874409 0.79661411 0.79693388 0.79672089 0.7997025  0.79906386\n",
      " 0.80055413 0.79991561 0.79810494 0.80023493 0.80044758 0.79885019\n",
      " 0.79938279 0.80002182 0.80044769 0.79991549 0.79927629 0.80044769\n",
      " 0.7992764  0.80002188 0.79895725 0.80193841 0.7992764  0.79948923\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.80598476 0.80417472 0.80555889 0.80577211\n",
      " 0.80577205 0.80545262 0.80683639 0.80470737 0.80598499 0.80609137\n",
      " 0.80662374 0.80460059 0.8068369  0.80523968 0.80555917 0.80609143\n",
      " 0.80577211 0.80523974 0.80481387 0.80577183 0.80428093 0.80492048\n",
      " 0.80651747 0.80523951 0.80417478 0.80513336 0.80470715 0.80598471\n",
      " 0.80587872 0.80566578 0.80460071 0.80598493 0.80641109 0.80555934\n",
      " 0.80662397 0.80566556 0.80502664 0.80641097 0.80566533 0.80598493\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.80694328 0.80747554 0.80673024 0.80811446\n",
      " 0.80641092 0.80577217 0.8075817  0.80736916 0.80854062 0.80641086\n",
      " 0.80673029 0.80811435 0.80683702 0.80768876 0.80683707 0.80736893\n",
      " 0.80715616 0.80736938 0.8072626  0.80726266 0.80747611 0.80641092\n",
      " 0.80758221 0.80822118 0.80662368 0.80779492 0.80907304 0.80747554\n",
      " 0.80683662 0.80662397 0.80726272 0.80811463 0.80609148 0.80683673\n",
      " 0.80790169 0.80694328 0.80736933 0.80683679 0.80673029 0.80715628\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79704055 0.79586886 0.79725348 0.79757263\n",
      " 0.79576237 0.79757303 0.7974658  0.79789235 0.79714687 0.79714682\n",
      " 0.79874392 0.79704021 0.79767953 0.79767919 0.79821155 0.79789201\n",
      " 0.79799873 0.7986376  0.79735953 0.79885008 0.79874381 0.80066068\n",
      " 0.79938273 0.79959544 0.80108633 0.79916991 0.79970228 0.80087379\n",
      " 0.80044741 0.79863794 0.79906341 0.79895674 0.80076718 0.79970233\n",
      " 0.79874375 0.80044775 0.80002205 0.79927657 0.79980888 0.79906347\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.80523974 0.8066238  0.80534601 0.80460048\n",
      " 0.80619798 0.80683673 0.80555889 0.80545245 0.80492008 0.80566578\n",
      " 0.80523963 0.80534601 0.80619804 0.80673035 0.80619787 0.80598465\n",
      " 0.80598493 0.80523951 0.80651719 0.80630414 0.80406885 0.80587827\n",
      " 0.80577222 0.80598499 0.80577211 0.80587844 0.80598499 0.80662408\n",
      " 0.80545262 0.80566527 0.80502647 0.80630459 0.80481376 0.80555878\n",
      " 0.80598493 0.80460065 0.80630476 0.80513296 0.80641075 0.80630436\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.80683656 0.80641086 0.80758192 0.80790158\n",
      " 0.8082213  0.80577188 0.80747543 0.80758215 0.80704967 0.80811469\n",
      " 0.80843401 0.80715616 0.80694334 0.80704984 0.80736927 0.8073691\n",
      " 0.80715628 0.80800802 0.80779537 0.80790158 0.80779514 0.80641126\n",
      " 0.80598493 0.80736916 0.8072626  0.80651747 0.80779514 0.80715622\n",
      " 0.80843423 0.80662385 0.80726255 0.80715588 0.8082213  0.80630459\n",
      " 0.80726266 0.80673046 0.80758232 0.8061977  0.80651736 0.80694323\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79704049 0.79672072 0.79629473 0.79725314\n",
      " 0.79650807 0.79693371 0.79895714 0.7965079  0.79757263 0.79789184\n",
      " 0.79714659 0.79618824 0.798105   0.79938307 0.79938296 0.79970222\n",
      " 0.79767924 0.79789201 0.7976793  0.7979985  0.7986372  0.79863737\n",
      " 0.7997025  0.79874398 0.79746642 0.80119283 0.80066034 0.79991521\n",
      " 0.79959561 0.80023504 0.79906364 0.79970239 0.79863731 0.79959606\n",
      " 0.79916985 0.80034165 0.79948917 0.79959595 0.79938307 0.79959589\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.80555929 0.80470703 0.80502647 0.80523968\n",
      " 0.80470703 0.80523968 0.805559   0.80577211 0.80641103 0.80662391\n",
      " 0.805772   0.80587821 0.80534624 0.8066238  0.80406851 0.80523951\n",
      " 0.80630453 0.80566544 0.80555906 0.80460082 0.8068369  0.80641063\n",
      " 0.80481387 0.805559   0.80545256 0.80683668 0.80598499 0.80609137\n",
      " 0.80523963 0.80555889 0.80566584 0.80619775 0.80502658 0.805772\n",
      " 0.8056655  0.80630448 0.80598516 0.80545228 0.80555895 0.805559\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.80630448 0.80673052 0.80726272 0.80694323\n",
      " 0.80673041 0.8082209  0.80854056 0.80811463 0.8074756  0.80790175\n",
      " 0.80832745 0.80758192 0.80715611 0.80704995 0.80736927 0.80768865\n",
      " 0.80704961 0.80726277 0.80811463 0.80726277 0.80726306 0.80758226\n",
      " 0.80736967 0.80715599 0.80619815 0.80704978 0.80747571 0.8081148\n",
      " 0.80715605 0.80694323 0.80704972 0.80694345 0.80790158 0.80715622\n",
      " 0.8065173  0.80811469 0.80736921 0.80726283 0.80715633 0.80758215]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.66490067 0.67259183 0.67186721 0.67451086\n",
      " 0.67090711 0.67379144 0.67426932 0.66634096 0.67475038 0.67402865\n",
      " 0.67234913 0.67498903 0.67138817 0.67691298 0.66730279 0.67523375\n",
      " 0.67379086 0.67258894 0.67427048 0.67378855 0.67090682 0.67451086\n",
      " 0.67667375 0.67258836 0.67450855 0.67354875 0.67306971 0.67282557\n",
      " 0.67667288 0.67307    0.67331009 0.67619096 0.67715278 0.67402923\n",
      " 0.67138702 0.67306653 0.67667201 0.67210932 0.67667375 0.67307\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68532096 0.67955461 0.68435942 0.67979586\n",
      " 0.68099548 0.67979269 0.68435884 0.68195615 0.67643134 0.68027288\n",
      " 0.6838798  0.67691298 0.67931538 0.68435971 0.68147567 0.68003653\n",
      " 0.68075509 0.68243894 0.67523259 0.67931336 0.6769124  0.68027576\n",
      " 0.67739057 0.6809949  0.68147653 0.67859394 0.6807574  0.67979673\n",
      " 0.67811201 0.67811403 0.67835384 0.6778725  0.68051615 0.68027317\n",
      " 0.68003567 0.68099807 0.68267701 0.67883519 0.67979442 0.68027432\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.6781123  0.68363855 0.67811317 0.68435999\n",
      " 0.68627961 0.68411759 0.68219538 0.68484076 0.68243374 0.68579999\n",
      " 0.68387865 0.68051701 0.68652086 0.68099519 0.68123673 0.68195672\n",
      " 0.68315749 0.68123673 0.68123846 0.68315721 0.67979557 0.68387663\n",
      " 0.68604038 0.68315836 0.68676009 0.68147855 0.68531922 0.6841199\n",
      " 0.68003509 0.68267672 0.68147682 0.68387807 0.68315749 0.6838772\n",
      " 0.68603807 0.68436144 0.68411903 0.68459922 0.68339846 0.68459836\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.67475154 0.66946481 0.67330721 0.67138817\n",
      " 0.67547067 0.67378913 0.67066817 0.67619125 0.66802452 0.67186692\n",
      " 0.67379173 0.67499192 0.6694674  0.67283019 0.67211134 0.67354961\n",
      " 0.66778442 0.67691298 0.67355019 0.67474865 0.66802481 0.6764348\n",
      " 0.67186865 0.67595115 0.67451086 0.67715394 0.67595173 0.67787625\n",
      " 0.67211019 0.6742725  0.67595404 0.67162654 0.67186836 0.67595202\n",
      " 0.67523202 0.67403009 0.67258836 0.67547182 0.67763153 0.67595404\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68003596 0.67835471 0.68099519 0.68028038\n",
      " 0.68075624 0.68051499 0.67979355 0.68003653 0.67643048 0.68171663\n",
      " 0.68027836 0.67955519 0.68075538 0.67715192 0.67931509 0.67811259\n",
      " 0.67859653 0.6788323  0.67787423 0.68147798 0.6793148  0.6740298\n",
      " 0.6807548  0.68243634 0.68099374 0.68003451 0.68147769 0.67907384\n",
      " 0.68171721 0.67739375 0.68267788 0.68171807 0.68099894 0.67979673\n",
      " 0.67955519 0.68027663 0.68051615 0.68195672 0.67979644 0.68051673\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68339846 0.67931451 0.68363855 0.68051528\n",
      " 0.67979355 0.68075596 0.68051471 0.68844451 0.6800348  0.68243692\n",
      " 0.68123557 0.68195672 0.68484047 0.68315749 0.68171663 0.68315692\n",
      " 0.68099634 0.68171749 0.68339961 0.68195759 0.68099519 0.68171692\n",
      " 0.68411846 0.68676355 0.68603922 0.68387605 0.68603749 0.68219682\n",
      " 0.68483845 0.68508057 0.68243836 0.68700163 0.68436115 0.68147653\n",
      " 0.68171894 0.68651999 0.68171576 0.68340019 0.68387922 0.68411759\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.67259038 0.66682057 0.66874509 0.66970779\n",
      " 0.67115067 0.67066615 0.67138846 0.67138817 0.67162827 0.6740298\n",
      " 0.67355106 0.6697075  0.67282759 0.67379029 0.67571048 0.6778748\n",
      " 0.67667115 0.67523259 0.67403298 0.67163    0.67162596 0.67307\n",
      " 0.66994615 0.67042461 0.67523057 0.67450855 0.6733098  0.68123586\n",
      " 0.67306827 0.67475182 0.6759523  0.67451057 0.67475009 0.67571163\n",
      " 0.67523    0.67571077 0.6723474  0.67090654 0.67595144 0.67379115\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.67643163 0.67691471 0.68532038 0.68051499\n",
      " 0.67667288 0.68075509 0.68027605 0.67883317 0.6788349  0.68003538\n",
      " 0.67979442 0.6819573  0.68267701 0.68075624 0.68027461 0.67691413\n",
      " 0.67667346 0.68003538 0.67763211 0.68099692 0.67859163 0.67787509\n",
      " 0.67955346 0.67955288 0.67787394 0.68219596 0.67979297 0.68315778\n",
      " 0.67787336 0.67907673 0.68460067 0.67883519 0.67955634 0.67931682\n",
      " 0.68099576 0.67859423 0.67763326 0.6843574  0.67835384 0.67931451\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68171721 0.68531922 0.6788323  0.68435826\n",
      " 0.68555845 0.68267701 0.67883432 0.68147769 0.68339615 0.68219682\n",
      " 0.68363682 0.68436086 0.68291942 0.68219798 0.68243374 0.68267759\n",
      " 0.6843574  0.67859394 0.68411788 0.68387605 0.68532211 0.68603951\n",
      " 0.68411817 0.68075711 0.68411846 0.68483961 0.68532124 0.68387894\n",
      " 0.68195701 0.68435826 0.68171634 0.68291624 0.68339903 0.68748211\n",
      " 0.68003394 0.68435999 0.68363769 0.68051586 0.68291797 0.68339961]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.76598764 0.75848477 0.76598374 0.77032689\n",
      " 0.76558692 0.76361298 0.76361376 0.76480342 0.76519166 0.76361531\n",
      " 0.76361531 0.76637821 0.76400823 0.76400979 0.76204052 0.76440661\n",
      " 0.76440427 0.76203585 0.76321928 0.76479953 0.76598296 0.76835294\n",
      " 0.76362077 0.76440661 0.76677737 0.76716873 0.77032767 0.7675671\n",
      " 0.76598608 0.76677503 0.76874664 0.7651979  0.76480031 0.76716873\n",
      " 0.76795768 0.76558848 0.76716561 0.76558848 0.76993241 0.76203818\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77229849 0.77111584 0.77151266 0.76795768\n",
      " 0.77032533 0.77150954 0.77111506 0.77151032 0.77151032 0.77387796\n",
      " 0.76559004 0.77111662 0.77427244 0.77545665 0.77466692 0.77309135\n",
      " 0.77229849 0.76874508 0.77150954 0.77032377 0.77308979 0.77427322\n",
      " 0.77111662 0.77427634 0.77269141 0.76874586 0.77230161 0.77150876\n",
      " 0.77230161 0.77230161 0.77071824 0.77072214 0.77071824 0.77388186\n",
      " 0.77506061 0.76914189 0.77071902 0.77269687 0.77229771 0.76874664\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77190324 0.77466614 0.76835372 0.7715111\n",
      " 0.77427166 0.77150876 0.77743138 0.77427244 0.77466925 0.77348115\n",
      " 0.77387796 0.77427322 0.77269453 0.77506295 0.77427322 0.77229927\n",
      " 0.77308823 0.77269453 0.77111584 0.7738764  0.77190168 0.77505905\n",
      " 0.77466614 0.7742701  0.77269297 0.7734827  0.77111428 0.77308823\n",
      " 0.77624483 0.77229615 0.77940298 0.77664086 0.7719048  0.77151344\n",
      " 0.77309057 0.77388108 0.77229849 0.77427166 0.77229849 0.77308979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.76204208 0.76361765 0.76756243 0.76558692\n",
      " 0.76480031 0.76795846 0.7679608  0.76874742 0.76322396 0.76598452\n",
      " 0.76558692 0.75966664 0.76717029 0.76677503 0.76638367 0.76401057\n",
      " 0.76637899 0.76282714 0.76637821 0.76243266 0.76479875 0.76677659\n",
      " 0.76914112 0.7632224  0.76677425 0.76440271 0.76479797 0.76993241\n",
      " 0.76716717 0.76716405 0.76677035 0.76637588 0.76874976 0.76795534\n",
      " 0.7663751  0.76874742 0.76440505 0.76874508 0.76795456 0.76637899\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.76953793 0.76637821 0.77308589 0.77072214\n",
      " 0.76914345 0.77071902 0.7719048  0.77427322 0.77032455 0.7742701\n",
      " 0.77190557 0.76874742 0.76716717 0.7738764  0.76953715 0.7746677\n",
      " 0.77230239 0.77111584 0.77072214 0.77230005 0.77269453 0.76716873\n",
      " 0.77190557 0.77506373 0.77466614 0.77426932 0.7719048  0.76835216\n",
      " 0.77427244 0.77111428 0.7687482  0.77151188 0.76953481 0.7734827\n",
      " 0.77111584 0.77072136 0.77427244 0.7734866  0.77427244 0.76953559\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77269297 0.77269297 0.76756243 0.77308823\n",
      " 0.77111818 0.76993085 0.77071824 0.77427088 0.77269063 0.77545587\n",
      " 0.77467003 0.77466536 0.77229927 0.77624638 0.77545743 0.77387718\n",
      " 0.77230005 0.7734827  0.77624638 0.77585191 0.76993319 0.76795379\n",
      " 0.7719048  0.77190324 0.77545899 0.77506373 0.77427322 0.77388264\n",
      " 0.77229849 0.774274   0.7738803  0.77545509 0.77308667 0.77427322\n",
      " 0.77230161 0.77624483 0.77505983 0.77387874 0.77506295 0.77506217\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.76756321 0.75967132 0.76479875 0.76677191\n",
      " 0.76638367 0.76321928 0.7655916  0.76519712 0.76716951 0.76519712\n",
      " 0.76322318 0.76440661 0.76164215 0.76125079 0.76164371 0.76361687\n",
      " 0.76677269 0.76440817 0.76203974 0.76440427 0.76361609 0.76598452\n",
      " 0.76401057 0.76756554 0.76598218 0.76716639 0.76598296 0.7687482\n",
      " 0.76361298 0.76835372 0.7687482  0.7655877  0.77150876 0.76598452\n",
      " 0.76558926 0.76480264 0.76598452 0.76203663 0.7711135  0.76756321\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77190557 0.76953715 0.77190246 0.76795846\n",
      " 0.77664164 0.77269375 0.77151266 0.76874976 0.77348115 0.7719048\n",
      " 0.77190869 0.77585035 0.77269375 0.77111506 0.77150954 0.77150954\n",
      " 0.77545743 0.77190402 0.77269141 0.77269219 0.77426854 0.76953325\n",
      " 0.77071824 0.77427166 0.77151032 0.76795612 0.7719048  0.7734827\n",
      " 0.7738803  0.76992929 0.77545665 0.77230083 0.77190402 0.77308979\n",
      " 0.77111428 0.77190168 0.77348426 0.77387796 0.7738764  0.77269375\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77348192 0.77427244 0.7738764  0.77387796\n",
      " 0.7715111  0.76953403 0.77743293 0.76953559 0.77268751 0.77348504\n",
      " 0.77072136 0.77190168 0.77308979 0.77150798 0.77309057 0.76874976\n",
      " 0.77545821 0.77229849 0.76914189 0.77467003 0.77229849 0.7734827\n",
      " 0.77072292 0.77150954 0.77230161 0.77505983 0.77506373 0.77348348\n",
      " 0.77230005 0.77427166 0.76954027 0.77664164 0.77308667 0.77545821\n",
      " 0.7715072  0.7770369  0.77269297 0.77466536 0.7766393  0.77624638]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.7351842  0.73693612 0.73342735 0.73448306\n",
      " 0.73342612 0.73553384 0.73728454 0.73377946 0.73167358 0.72851691\n",
      " 0.73693489 0.73588349 0.73518112 0.73202569 0.73729069 0.7330777\n",
      " 0.73939103 0.73623375 0.73272683 0.73834209 0.72921682 0.7362356\n",
      " 0.73272375 0.73869358 0.73974191 0.73482902 0.73623498 0.73693244\n",
      " 0.73763296 0.7348327  0.7351842  0.73343042 0.7348327  0.737287\n",
      " 0.73763665 0.73342612 0.73763849 0.73623498 0.73588472 0.73377761\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74219682 0.74254831 0.73868805 0.74149384\n",
      " 0.74500261 0.74394937 0.73763788 0.74675884 0.74044797 0.74149507\n",
      " 0.745002   0.74254647 0.73938981 0.74079639 0.74850954 0.74219867\n",
      " 0.74009463 0.7432513  0.73798753 0.74360279 0.74570375 0.74535103\n",
      " 0.73904323 0.73939656 0.74395182 0.73974191 0.74044367 0.7436034\n",
      " 0.73868682 0.73938919 0.737287   0.7414963  0.74324884 0.74149445\n",
      " 0.7432513  0.74430024 0.74219682 0.74465419 0.73904139 0.73833902\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74219498 0.74360033 0.74289919 0.74079762\n",
      " 0.74289735 0.74500261 0.74535287 0.7467527  0.74815928 0.74324945\n",
      " 0.74114296 0.74219437 0.7435991  0.74149323 0.74044428 0.73939165\n",
      " 0.74570252 0.74254586 0.74149507 0.74430639 0.74219867 0.7407927\n",
      " 0.745002   0.73973945 0.7435991  0.7414963  0.74535226 0.74430393\n",
      " 0.74044182 0.73868744 0.74324823 0.74184533 0.74359726 0.74710665\n",
      " 0.74184717 0.74570375 0.74570559 0.74430024 0.74289796 0.74114481\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.73588349 0.73413402 0.73167419 0.73237533\n",
      " 0.73131963 0.73377761 0.73693797 0.73693735 0.73868805 0.73798875\n",
      " 0.7362313  0.73447998 0.7379906  0.73062156 0.73167481 0.73728516\n",
      " 0.733777   0.73798691 0.73307893 0.73588411 0.73413402 0.73729069\n",
      " 0.739042   0.73764095 0.73588472 0.73834025 0.73062402 0.73167665\n",
      " 0.73448121 0.73728823 0.73342735 0.73483025 0.73518174 0.73798937\n",
      " 0.73588288 0.73658525 0.73693735 0.73763603 0.73869051 0.73378069\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74184472 0.73869112 0.73588411 0.73834025\n",
      " 0.74324884 0.7418484  0.74290104 0.74079516 0.74184595 0.74184656\n",
      " 0.74360033 0.74220051 0.74079393 0.74465296 0.74219744 0.7435991\n",
      " 0.73869174 0.74009463 0.74254893 0.73904139 0.7390377  0.74254586\n",
      " 0.73973761 0.74079454 0.73623314 0.73868989 0.74079577 0.74219682\n",
      " 0.74254647 0.74219744 0.74430147 0.73693612 0.74114296 0.74395244\n",
      " 0.74114788 0.74360095 0.7436034  0.74219621 0.74114542 0.74394752\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74009648 0.74605524 0.74570375 0.74219498\n",
      " 0.74044367 0.7443027  0.74149384 0.74430147 0.74254709 0.74675823\n",
      " 0.74851015 0.74394937 0.74395182 0.74535226 0.74359972 0.74535287\n",
      " 0.74219682 0.74289796 0.74360156 0.74079209 0.7414963  0.74149445\n",
      " 0.73938919 0.74149753 0.74149691 0.74114542 0.74114542 0.74464928\n",
      " 0.74500384 0.74535042 0.74079086 0.74254954 0.74219867 0.74079762\n",
      " 0.74710665 0.7400934  0.74394998 0.74219805 0.74254893 0.74500261\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.7330777  0.73518297 0.73939472 0.73202569\n",
      " 0.73062341 0.73377823 0.73693674 0.73763665 0.73553446 0.7348284\n",
      " 0.73623375 0.73623253 0.73553016 0.73518727 0.73412911 0.73693797\n",
      " 0.73448121 0.73658156 0.73307709 0.73658525 0.73483393 0.73798937\n",
      " 0.73623928 0.74044428 0.73623314 0.73553261 0.73763665 0.73798814\n",
      " 0.73447937 0.73763911 0.73763788 0.73624051 0.73868621 0.73763849\n",
      " 0.73308078 0.73518051 0.74079331 0.73623375 0.73588595 0.73307955\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74289919 0.7404449  0.73623437 0.73939226\n",
      " 0.74254709 0.74114419 0.7404449  0.73869235 0.73869051 0.74535349\n",
      " 0.73974498 0.74289796 0.74465173 0.74114481 0.74219744 0.7414963\n",
      " 0.74324884 0.74430332 0.74360156 0.74149445 0.7453541  0.74149507\n",
      " 0.74289735 0.73483147 0.73938858 0.74114726 0.74465235 0.73763911\n",
      " 0.74009525 0.74044674 0.74009217 0.74114726 0.7400891  0.74114481\n",
      " 0.74079516 0.73833902 0.74184717 0.7414963  0.7404449  0.73904077\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74745445 0.737287   0.7460534  0.73868928\n",
      " 0.74500138 0.74464928 0.74184533 0.74640489 0.74359972 0.74956156\n",
      " 0.74359849 0.74430209 0.74009463 0.74079454 0.74780656 0.74535287\n",
      " 0.73833902 0.74184656 0.74184472 0.74219928 0.73868989 0.74324945\n",
      " 0.74324761 0.74394691 0.74114481 0.74219744 0.74429963 0.74921314\n",
      " 0.74184656 0.7435991  0.74745629 0.74219621 0.74114419 0.74500261\n",
      " 0.74114481 0.74184595 0.74394998 0.74465173 0.74044428 0.74640551]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.6820642  0.6791649  0.68322352 0.6820642\n",
      " 0.68080806 0.68467317 0.67916438 0.68119412 0.68322352 0.68409257\n",
      " 0.68158068 0.68119444 0.68235358 0.68225762 0.68322343 0.68129106\n",
      " 0.67964785 0.68351314 0.68380342 0.6818704  0.68447956 0.68515589\n",
      " 0.68370638 0.6848666  0.68525312 0.6860255  0.68196753 0.68438369\n",
      " 0.68718492 0.68467336 0.68689502 0.68554302 0.68447993 0.68544649\n",
      " 0.68544617 0.68612235 0.68351291 0.68399633 0.68583264 0.68477021\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.6982031  0.69926614 0.6995552  0.70052227\n",
      " 0.69849217 0.69540056 0.7006181  0.6995559  0.69974895 0.69829968\n",
      " 0.69878174 0.70158461 0.6990729  0.69974862 0.69887892 0.69945933\n",
      " 0.69926563 0.69810625 0.70052185 0.69733312 0.70119897 0.69984529\n",
      " 0.69839639 0.69974928 0.69762316 0.69887906 0.6974296  0.70139132\n",
      " 0.69829949 0.70023181 0.69820264 0.69945938 0.70013589 0.69955543\n",
      " 0.69694632 0.69849268 0.69965224 0.69887943 0.70110208 0.69916919\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70245525 0.70274459 0.70525769 0.70245525\n",
      " 0.70496765 0.70574088 0.70438794 0.70216497 0.70206845 0.70419433\n",
      " 0.70409743 0.70458099 0.70332453 0.70274482 0.70448446 0.70332444\n",
      " 0.70390471 0.70168146 0.70458071 0.70371124 0.70496695 0.70197183\n",
      " 0.70535403 0.70400109 0.70409799 0.70342106 0.7032281  0.70148855\n",
      " 0.70226187 0.70419428 0.70487089 0.70148831 0.70235844 0.70342096\n",
      " 0.70506385 0.70409748 0.7030351  0.70506366 0.70322805 0.70235849\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68138717 0.67858439 0.68303038 0.68438276\n",
      " 0.68100093 0.68003451 0.6810975  0.68264367 0.68129078 0.67964804\n",
      " 0.68380314 0.68457683 0.68206396 0.68303047 0.68235368 0.68332037\n",
      " 0.68380295 0.68167754 0.68312695 0.68341671 0.68293357 0.68409281\n",
      " 0.68361051 0.68477007 0.68457604 0.68196734 0.68515603 0.68525382\n",
      " 0.68409332 0.68418989 0.68399666 0.68534955 0.68554362 0.68534978\n",
      " 0.68641258 0.6864124  0.68612315 0.68496321 0.68631596 0.68515594\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.6969465  0.6986861  0.69926563 0.70216464\n",
      " 0.70071527 0.69974839 0.69829968 0.69839541 0.69858962 0.69858962\n",
      " 0.70216455 0.70032871 0.6997489  0.69926567 0.70090842 0.69887869\n",
      " 0.69723631 0.69994219 0.69800926 0.69994214 0.69849165 0.70013482\n",
      " 0.69974858 0.69916873 0.70061786 0.699749   0.70023176 0.69984552\n",
      " 0.69994223 0.69955553 0.69945891 0.70052232 0.69955599 0.69916905\n",
      " 0.69984576 0.70148855 0.7016815  0.69781635 0.69974895 0.69974937\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70168183 0.70071537 0.7049669  0.70419456\n",
      " 0.70419456 0.70554708 0.70148869 0.70554689 0.70187535 0.70226192\n",
      " 0.70158535 0.7026483  0.70400095 0.70351809 0.70380818 0.70274496\n",
      " 0.70351772 0.70293852 0.70400091 0.70235835 0.70235863 0.7057405\n",
      " 0.70303463 0.70545018 0.70284144 0.70332462 0.70525718 0.70496732\n",
      " 0.70303435 0.70322819 0.70400128 0.70361411 0.70303458 0.70390466\n",
      " 0.70303472 0.70467723 0.70429127 0.70467747 0.70313162 0.70622374\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.6828371  0.67790783 0.68148425 0.68187017\n",
      " 0.68167754 0.67935762 0.6835138  0.68090431 0.68650892 0.68409374\n",
      " 0.68254715 0.68167688 0.68254687 0.68032441 0.68177383 0.68100111\n",
      " 0.68245048 0.68109713 0.68080802 0.68225706 0.68699243 0.68486585\n",
      " 0.6839006  0.68438383 0.68438299 0.68534936 0.68293413 0.68418961\n",
      " 0.6833207  0.68728205 0.68602653 0.68515566 0.68476988 0.68592921\n",
      " 0.68515622 0.68708928 0.68505955 0.68467275 0.68602564 0.68554283\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70168169 0.69994279 0.7001358  0.69955539\n",
      " 0.69887901 0.69936285 0.70052293 0.70158451 0.70100462 0.69936201\n",
      " 0.69887911 0.69839587 0.70081236 0.70023181 0.70148841 0.69984566\n",
      " 0.69887897 0.70013556 0.69916901 0.69781621 0.70052157 0.69781663\n",
      " 0.6987816  0.69771969 0.69810602 0.69926544 0.69926525 0.70003834\n",
      " 0.69897568 0.69984538 0.70061856 0.70100541 0.69907272 0.69907234\n",
      " 0.69955543 0.69868592 0.69955609 0.70061866 0.70119813 0.69955525\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70187512 0.70400105 0.70235811 0.70467761\n",
      " 0.70322819 0.70235881 0.70400142 0.70197164 0.70284167 0.70429081\n",
      " 0.70284139 0.70351795 0.70351795 0.70274491 0.70187455 0.70245501\n",
      " 0.70419447 0.70371072 0.70390475 0.7033243  0.70371072 0.70486986\n",
      " 0.70235825 0.7040978  0.7055468  0.70641721 0.70351791 0.70371133\n",
      " 0.70380767 0.70303477 0.70274463 0.70235802 0.70293834 0.70197192\n",
      " 0.70400109 0.70506408 0.70419461 0.70226201 0.70477451 0.70226131]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/bhaveshshah/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68586597 0.68436582 0.68356181 0.68570533\n",
      " 0.68479425 0.68463369 0.6868303  0.68291925 0.68640176 0.68629455\n",
      " 0.68608031 0.68441926 0.68725918 0.68516935 0.68506224 0.68586613\n",
      " 0.68758061 0.68790198 0.68527663 0.68565165 0.68715193 0.6890272\n",
      " 0.68790208 0.68768797 0.68693738 0.68875927 0.68961645 0.68779472\n",
      " 0.68865199 0.68833048 0.68827694 0.68961655 0.68956302 0.68913431\n",
      " 0.68865182 0.68849142 0.69106322 0.68977715 0.6901525  0.68881286\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70156463 0.70381492 0.70065371 0.7027435\n",
      " 0.70177882 0.70151096 0.7037079  0.70392186 0.70360055 0.70317206\n",
      " 0.7048329  0.70349339 0.70317192 0.7027434  0.70317186 0.70220762\n",
      " 0.70306488 0.70301111 0.70279696 0.70268981 0.70242177 0.7030649\n",
      " 0.70070736 0.70236842 0.70274321 0.70113608 0.70081464 0.70215407\n",
      " 0.70510085 0.70488649 0.70435071 0.70295766 0.70199324 0.70204682\n",
      " 0.70349348 0.70199319 0.70268978 0.70258247 0.70311845 0.70354698\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70702977 0.70724406 0.70713686 0.70761918\n",
      " 0.70531527 0.70633329 0.70724391 0.70697623 0.70794065 0.70702975\n",
      " 0.70702991 0.70649401 0.70590471 0.70761931 0.70638691 0.70697619\n",
      " 0.707512   0.70697622 0.70794058 0.70761929 0.70804774 0.70627956\n",
      " 0.70606545 0.70633303 0.70794048 0.70783341 0.70826211 0.70681556\n",
      " 0.70681548 0.70794062 0.70724407 0.70499381 0.70681556 0.70611916\n",
      " 0.7063869  0.70708349 0.70719047 0.70729772 0.70590482 0.70665479\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68672328 0.6850087  0.68516927 0.68629461\n",
      " 0.68559816 0.68441939 0.68795556 0.68270469 0.68420512 0.68549072\n",
      " 0.68650893 0.68645538 0.68634806 0.68527681 0.68645535 0.68827692\n",
      " 0.68565161 0.68784838 0.68602683 0.68533012 0.68731262 0.68838413\n",
      " 0.68758054 0.68881272 0.68924159 0.68929521 0.68838411 0.68683056\n",
      " 0.68774127 0.68827694 0.68897368 0.68758068 0.69058091 0.68854473\n",
      " 0.69074157 0.68758051 0.68779492 0.68892006 0.69009878 0.68945562\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70193958 0.70033231 0.70161808 0.70172528\n",
      " 0.70140386 0.70338638 0.70370786 0.70327894 0.70451156 0.7026364\n",
      " 0.70258254 0.70167187 0.70349354 0.70349355 0.70317196 0.70172533\n",
      " 0.70322557 0.70429708 0.70301141 0.70124312 0.7023148  0.70429712\n",
      " 0.703065   0.70263635 0.70349354 0.70220761 0.70263616 0.70327913\n",
      " 0.70295756 0.70177911 0.701779   0.70306468 0.70354707 0.70349346\n",
      " 0.70108258 0.70520803 0.70258251 0.70204689 0.70220769 0.70215384\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70638681 0.70713685 0.70654751 0.70547587\n",
      " 0.7066011  0.70606541 0.70686905 0.70804772 0.70670818 0.7062261\n",
      " 0.70697629 0.70740493 0.70702985 0.70724424 0.70799427 0.70670821\n",
      " 0.70713683 0.70644041 0.70735125 0.70847638 0.70826203 0.70611894\n",
      " 0.70579753 0.70611879 0.70713686 0.70633312 0.70719052 0.7064404\n",
      " 0.70719057 0.70644037 0.70751207 0.70515459 0.70767255 0.7069228\n",
      " 0.7076727  0.70810141 0.70627977 0.70729772 0.70702994 0.70611891\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.68597327 0.68688376 0.68356178 0.68575872\n",
      " 0.68463342 0.68624099 0.6861337  0.68570517 0.6866697  0.68538356\n",
      " 0.68613384 0.68506216 0.68602666 0.68645531 0.68672319 0.68597312\n",
      " 0.68425853 0.68608042 0.6857051  0.68618731 0.68672323 0.68763402\n",
      " 0.68817001 0.68929494 0.68908073 0.68827702 0.68929502 0.68886657\n",
      " 0.68875935 0.69106298 0.68688396 0.69068803 0.68956297 0.68892\n",
      " 0.6875806  0.69165261 0.68929502 0.68945566 0.6880626  0.68875939\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70001081 0.70135037 0.70322551 0.70226101\n",
      " 0.70413628 0.70161827 0.70258244 0.70408276 0.70129677 0.70381484\n",
      " 0.7034398  0.7018327  0.7027435  0.7040294  0.70322547 0.70440435\n",
      " 0.70386861 0.70349351 0.70306478 0.70333271 0.70204679 0.70258253\n",
      " 0.70199329 0.70220745 0.70242204 0.70258258 0.70290405 0.70333259\n",
      " 0.7034399  0.70381485 0.70274313 0.70386843 0.70408293 0.70408296\n",
      " 0.70349352 0.703708   0.70365421 0.70220758 0.70333269 0.70456522\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.7054224  0.70515433 0.7066546  0.70697613\n",
      " 0.70649393 0.70595829 0.70638688 0.70563674 0.70783337 0.7062261\n",
      " 0.7068691  0.70772634 0.70745848 0.70767268 0.70810162 0.70654748\n",
      " 0.70772627 0.70574385 0.70820857 0.70729768 0.70558295 0.7048866\n",
      " 0.70638682 0.70601189 0.70617258 0.70622595 0.70686894 0.70772634\n",
      " 0.70740487 0.70627983 0.70751208 0.70611884 0.70729771 0.70676179\n",
      " 0.70611899 0.70767278 0.70735104 0.70751211 0.70842294 0.70863728]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cluster0': 0.8114285714285714, 'Cluster1': 0.6834733893557423, 'Cluster2': 0.7688766114180479, 'Cluster3': 0.7579721995094031, 'Cluster4': 0.6965050732807215, 'Cluster5': 0.7074634329291162}\n",
      "{'Cluster0': 160, 'Cluster1': 170, 'Cluster2': 200, 'Cluster3': 190, 'Cluster4': 150, 'Cluster5': 290}\n"
     ]
    }
   ],
   "source": [
    "clusters = [\"Cluster0\", \"Cluster1\", \"Cluster2\", \"Cluster3\", \"Cluster4\", \"Cluster5\"]\n",
    "\n",
    "accuracyDict = {}\n",
    "numEstimatorsDict = {}\n",
    "\n",
    "for clust in clusters:\n",
    "    # Preprocessing\n",
    "    path_k = \"../Data/\" + clust\n",
    "    df = pd.read_csv(path_k, delimiter=',')\n",
    "    df_feat = df.iloc[:,2:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_feat)\n",
    "    X = pd.DataFrame(scaled_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    # Train model\n",
    "    RF = RandomForestClassifier(oob_score=True)\n",
    "    gs = GridSearchCV(RF, grid_params, verbose= 1, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "#     RF = RandomForestClassifier(n_estimators = gs.best_params_[\"n_estimators\"])\n",
    "#     RF.fit(X_train, y_train)\n",
    "    \n",
    "    # Store accuracies\n",
    "    y_pred = gs.predict(X_test)\n",
    "    accuracyDict[clust] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store best # of estimtators\n",
    "    numEstimatorsDict[clust] = gs.best_params_[\"n_estimators\"]\n",
    "    \n",
    "print(accuracyDict)\n",
    "print(numEstimatorsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ibOoQ1Vstdgi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ibOoQ1Vstdgi",
    "outputId": "43a8d681-3dae-4de1-ebac-a2791589341e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  \\\n",
       "0           0   0  18393       2     168    62.0    110     80            1   \n",
       "1           1   1  20228       1     156    85.0    140     90            3   \n",
       "2           2   2  18857       1     165    64.0    130     70            3   \n",
       "3           3   3  17623       2     169    82.0    150    100            1   \n",
       "4           4   4  17474       1     156    56.0    100     60            1   \n",
       "\n",
       "   gluc  smoke  alco  active  cardio  label  \n",
       "0     1      0     0       1       0      2  \n",
       "1     1      0     0       1       1      0  \n",
       "2     1      0     0       0       1      3  \n",
       "3     1      0     0       1       1      0  \n",
       "4     1      0     0       0       0      3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entire dataset\n",
    "\n",
    "df = pd.read_csv(\"../data/cardio_train.csv\", delimiter = \",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9DfI3YuGtrre",
   "metadata": {
    "id": "9DfI3YuGtrre"
   },
   "outputs": [],
   "source": [
    "targ = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "yeVbFI7atw72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "yeVbFI7atw72",
    "outputId": "39351d96-04a4-43df-e873-151060ad139c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = df_feat = df.iloc[:,2:-1]\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20JFInjgt13e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20JFInjgt13e",
    "outputId": "802b16ae-9aea-443c-883f-8ecb14fbd4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.434078  1.367567  0.455073 -0.855933 -1.000902 -0.136704 -0.536680   \n",
      "1      0.309349 -0.731226 -1.074178  0.782998  0.812696  0.931849  2.410745   \n",
      "2     -0.246094 -0.731226  0.072760 -0.713417  0.208163 -1.205258  2.410745   \n",
      "3     -0.746033  1.367567  0.582510  0.569225  1.417228  2.000403 -0.536680   \n",
      "4     -0.806399 -0.731226 -1.074178 -1.283480 -1.605434 -2.273812 -0.536680   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "68499 -0.090927  1.367567  0.455073  0.141677 -0.396369 -0.136704 -0.536680   \n",
      "68500  1.270739 -0.731226 -0.819303  3.704570  0.812696  0.931849  0.937033   \n",
      "68501 -0.161420  1.367567  2.366636  2.208155  3.230825  0.931849  2.410745   \n",
      "68502  1.201866 -0.731226 -0.182115 -0.143354  0.510429 -0.136704 -0.536680   \n",
      "68503  0.435752 -0.731226  0.709948 -0.143354 -0.396369 -0.136704  0.937033   \n",
      "\n",
      "             7         8         9         10        11  \n",
      "0     -0.394639 -0.310620 -0.237475  0.494776 -0.988360  \n",
      "1     -0.394639 -0.310620 -0.237475  0.494776  1.011777  \n",
      "2     -0.394639 -0.310620 -0.237475 -2.021118  1.011777  \n",
      "3     -0.394639 -0.310620 -0.237475  0.494776  1.011777  \n",
      "4     -0.394639 -0.310620 -0.237475 -2.021118 -0.988360  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "68499 -0.394639  3.219363 -0.237475  0.494776 -0.988360  \n",
      "68500  1.355271 -0.310620 -0.237475  0.494776  1.011777  \n",
      "68501 -0.394639 -0.310620  4.210973 -2.021118  1.011777  \n",
      "68502  1.355271 -0.310620 -0.237475 -2.021118  1.011777  \n",
      "68503 -0.394639 -0.310620 -0.237475  0.494776 -0.988360  \n",
      "\n",
      "[68504 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_feat)\n",
    "X = pd.DataFrame(scaled_features)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "x3avAKqFt83o",
   "metadata": {
    "id": "x3avAKqFt83o"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "Ia9pN11SuEjI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ia9pN11SuEjI",
    "outputId": "261085ce-6bc5-4180-8762-b21f13332fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ...................................n_estimators=200; total time=  14.7s\n",
      "[CV] END ...................................n_estimators=200; total time=  14.5s\n",
      "[CV] END ...................................n_estimators=200; total time=  14.6s\n",
      "[CV] END ...................................n_estimators=200; total time=  14.6s\n",
      "[CV] END ...................................n_estimators=200; total time=  14.5s\n",
      "[CV] END ...................................n_estimators=210; total time=  15.3s\n",
      "[CV] END ...................................n_estimators=210; total time=  15.2s\n",
      "[CV] END ...................................n_estimators=210; total time=  15.3s\n",
      "[CV] END ...................................n_estimators=210; total time=  15.5s\n",
      "[CV] END ...................................n_estimators=210; total time=  15.6s\n",
      "[CV] END ...................................n_estimators=220; total time=  16.1s\n",
      "[CV] END ...................................n_estimators=220; total time=  16.1s\n",
      "[CV] END ...................................n_estimators=220; total time=  16.0s\n",
      "[CV] END ...................................n_estimators=220; total time=  16.1s\n",
      "[CV] END ...................................n_estimators=220; total time=  16.1s\n",
      "[CV] END ...................................n_estimators=230; total time=  16.9s\n",
      "[CV] END ...................................n_estimators=230; total time=  16.9s\n",
      "[CV] END ...................................n_estimators=230; total time=  17.0s\n",
      "[CV] END ...................................n_estimators=230; total time=  16.8s\n",
      "[CV] END ...................................n_estimators=230; total time=  16.7s\n",
      "[CV] END ...................................n_estimators=240; total time=  17.4s\n",
      "[CV] END ...................................n_estimators=240; total time=  17.7s\n",
      "[CV] END ...................................n_estimators=240; total time=  17.6s\n",
      "[CV] END ...................................n_estimators=240; total time=  17.5s\n",
      "[CV] END ...................................n_estimators=240; total time=  17.5s\n",
      "[CV] END ...................................n_estimators=250; total time=  18.1s\n",
      "[CV] END ...................................n_estimators=250; total time=  18.0s\n",
      "[CV] END ...................................n_estimators=250; total time=  18.1s\n",
      "[CV] END ...................................n_estimators=250; total time=  18.0s\n",
      "[CV] END ...................................n_estimators=250; total time=  18.3s\n",
      "[CV] END ...................................n_estimators=260; total time=  19.1s\n",
      "[CV] END ...................................n_estimators=260; total time=  18.8s\n",
      "[CV] END ...................................n_estimators=260; total time=  18.8s\n",
      "[CV] END ...................................n_estimators=260; total time=  19.0s\n",
      "[CV] END ...................................n_estimators=260; total time=  18.8s\n",
      "[CV] END ...................................n_estimators=270; total time=  19.6s\n",
      "[CV] END ...................................n_estimators=270; total time=  19.5s\n",
      "[CV] END ...................................n_estimators=270; total time=  19.6s\n",
      "[CV] END ...................................n_estimators=270; total time=  19.7s\n",
      "[CV] END ...................................n_estimators=270; total time=  19.6s\n",
      "[CV] END ...................................n_estimators=280; total time=  20.2s\n",
      "[CV] END ...................................n_estimators=280; total time=  20.2s\n",
      "[CV] END ...................................n_estimators=280; total time=  20.5s\n",
      "[CV] END ...................................n_estimators=280; total time=  20.6s\n",
      "[CV] END ...................................n_estimators=280; total time=  20.4s\n",
      "[CV] END ...................................n_estimators=290; total time=  21.2s\n",
      "[CV] END ...................................n_estimators=290; total time=  21.1s\n",
      "[CV] END ...................................n_estimators=290; total time=  21.3s\n",
      "[CV] END ...................................n_estimators=290; total time=  21.6s\n",
      "[CV] END ...................................n_estimators=290; total time=  21.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7248"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "gs = GridSearchCV(forest, grid_params, verbose= 2)\n",
    "gs.fit(X_train, y_train)\n",
    "forest = RandomForestClassifier(n_estimators=gs.best_params_[\"n_estimators\"])\n",
    "forest.fit(X_train, y_train)\n",
    "preds = forest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Random Forests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
